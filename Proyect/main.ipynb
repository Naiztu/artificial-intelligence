{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2.2 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**José Ángel Rico Mendieta - A01707404**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = './'\n",
    "df = pd.read_csv(ruta+'data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>beats</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117.453835</td>\n",
       "      <td>57</td>\n",
       "      <td>0.410990</td>\n",
       "      <td>0.207584</td>\n",
       "      <td>3301.781785</td>\n",
       "      <td>3175.656592</td>\n",
       "      <td>7302.597509</td>\n",
       "      <td>0.144527</td>\n",
       "      <td>-40.802069</td>\n",
       "      <td>61.522054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.568772</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>-1.981371</td>\n",
       "      <td>1.915755</td>\n",
       "      <td>0.076505</td>\n",
       "      <td>-1.270496</td>\n",
       "      <td>1.233082</td>\n",
       "      <td>1.493700</td>\n",
       "      <td>0.949931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.302557</td>\n",
       "      <td>38</td>\n",
       "      <td>0.360462</td>\n",
       "      <td>0.138195</td>\n",
       "      <td>2765.747194</td>\n",
       "      <td>3222.713605</td>\n",
       "      <td>6873.391771</td>\n",
       "      <td>0.096015</td>\n",
       "      <td>-119.320605</td>\n",
       "      <td>85.941142</td>\n",
       "      <td>...</td>\n",
       "      <td>1.566938</td>\n",
       "      <td>-2.932391</td>\n",
       "      <td>-1.609138</td>\n",
       "      <td>-0.200545</td>\n",
       "      <td>-2.429376</td>\n",
       "      <td>1.207988</td>\n",
       "      <td>5.848729</td>\n",
       "      <td>1.609469</td>\n",
       "      <td>7.580263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107.666016</td>\n",
       "      <td>50</td>\n",
       "      <td>0.454810</td>\n",
       "      <td>0.159711</td>\n",
       "      <td>3279.877315</td>\n",
       "      <td>3026.391118</td>\n",
       "      <td>6953.099622</td>\n",
       "      <td>0.172819</td>\n",
       "      <td>-35.277026</td>\n",
       "      <td>61.242109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.518891</td>\n",
       "      <td>0.212918</td>\n",
       "      <td>-1.294660</td>\n",
       "      <td>-0.105626</td>\n",
       "      <td>-1.245879</td>\n",
       "      <td>-2.528455</td>\n",
       "      <td>0.571032</td>\n",
       "      <td>1.569434</td>\n",
       "      <td>-1.724873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.359375</td>\n",
       "      <td>52</td>\n",
       "      <td>0.454698</td>\n",
       "      <td>0.223419</td>\n",
       "      <td>3124.176350</td>\n",
       "      <td>2970.320532</td>\n",
       "      <td>6618.134981</td>\n",
       "      <td>0.142721</td>\n",
       "      <td>-42.757179</td>\n",
       "      <td>62.799474</td>\n",
       "      <td>...</td>\n",
       "      <td>3.020176</td>\n",
       "      <td>-1.242321</td>\n",
       "      <td>-3.524069</td>\n",
       "      <td>-0.325165</td>\n",
       "      <td>-1.443294</td>\n",
       "      <td>-1.864617</td>\n",
       "      <td>-0.031307</td>\n",
       "      <td>-0.729631</td>\n",
       "      <td>-0.987611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.359375</td>\n",
       "      <td>51</td>\n",
       "      <td>0.451668</td>\n",
       "      <td>0.240985</td>\n",
       "      <td>3236.413205</td>\n",
       "      <td>2953.596262</td>\n",
       "      <td>6698.776160</td>\n",
       "      <td>0.164825</td>\n",
       "      <td>4.821123</td>\n",
       "      <td>64.495215</td>\n",
       "      <td>...</td>\n",
       "      <td>1.753352</td>\n",
       "      <td>3.336132</td>\n",
       "      <td>2.352307</td>\n",
       "      <td>1.428845</td>\n",
       "      <td>-0.604926</td>\n",
       "      <td>-2.185691</td>\n",
       "      <td>-1.602394</td>\n",
       "      <td>-0.612146</td>\n",
       "      <td>-4.050126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>143.554688</td>\n",
       "      <td>69</td>\n",
       "      <td>0.269065</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>1218.290508</td>\n",
       "      <td>1353.616224</td>\n",
       "      <td>2295.177788</td>\n",
       "      <td>0.069813</td>\n",
       "      <td>-300.937697</td>\n",
       "      <td>161.111154</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.808325</td>\n",
       "      <td>-4.104602</td>\n",
       "      <td>-2.045668</td>\n",
       "      <td>-1.596437</td>\n",
       "      <td>0.646648</td>\n",
       "      <td>1.454414</td>\n",
       "      <td>-0.221094</td>\n",
       "      <td>-1.292084</td>\n",
       "      <td>-7.100527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>117.453835</td>\n",
       "      <td>57</td>\n",
       "      <td>0.279421</td>\n",
       "      <td>0.020732</td>\n",
       "      <td>1284.921775</td>\n",
       "      <td>1659.622614</td>\n",
       "      <td>2476.568358</td>\n",
       "      <td>0.063750</td>\n",
       "      <td>-360.821769</td>\n",
       "      <td>138.999302</td>\n",
       "      <td>...</td>\n",
       "      <td>4.684600</td>\n",
       "      <td>3.343187</td>\n",
       "      <td>-2.251255</td>\n",
       "      <td>-1.852868</td>\n",
       "      <td>2.215029</td>\n",
       "      <td>1.784788</td>\n",
       "      <td>-1.519674</td>\n",
       "      <td>-3.253521</td>\n",
       "      <td>3.998950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>151.999081</td>\n",
       "      <td>76</td>\n",
       "      <td>0.281670</td>\n",
       "      <td>0.048189</td>\n",
       "      <td>1322.026416</td>\n",
       "      <td>1590.233329</td>\n",
       "      <td>2473.626709</td>\n",
       "      <td>0.076126</td>\n",
       "      <td>-274.426858</td>\n",
       "      <td>148.374498</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.592368</td>\n",
       "      <td>-1.540610</td>\n",
       "      <td>-3.028737</td>\n",
       "      <td>-4.697145</td>\n",
       "      <td>-7.715005</td>\n",
       "      <td>-7.212517</td>\n",
       "      <td>-5.410751</td>\n",
       "      <td>-5.246985</td>\n",
       "      <td>-5.270150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>135.999178</td>\n",
       "      <td>67</td>\n",
       "      <td>0.254774</td>\n",
       "      <td>0.034038</td>\n",
       "      <td>1516.626219</td>\n",
       "      <td>1629.500705</td>\n",
       "      <td>2973.782018</td>\n",
       "      <td>0.078788</td>\n",
       "      <td>-292.128300</td>\n",
       "      <td>127.769046</td>\n",
       "      <td>...</td>\n",
       "      <td>4.709107</td>\n",
       "      <td>0.377206</td>\n",
       "      <td>1.464333</td>\n",
       "      <td>3.393674</td>\n",
       "      <td>6.076538</td>\n",
       "      <td>0.963429</td>\n",
       "      <td>-0.753660</td>\n",
       "      <td>-2.105312</td>\n",
       "      <td>0.755132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>112.347147</td>\n",
       "      <td>25</td>\n",
       "      <td>0.245491</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>1001.400439</td>\n",
       "      <td>1151.028078</td>\n",
       "      <td>1731.722862</td>\n",
       "      <td>0.057948</td>\n",
       "      <td>-369.184224</td>\n",
       "      <td>171.986997</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.797995</td>\n",
       "      <td>-5.051084</td>\n",
       "      <td>-5.084386</td>\n",
       "      <td>-3.038442</td>\n",
       "      <td>-1.718458</td>\n",
       "      <td>-1.849090</td>\n",
       "      <td>-2.257478</td>\n",
       "      <td>-1.273307</td>\n",
       "      <td>-0.441167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tempo  beats  chroma_stft      rmse  spectral_centroid  \\\n",
       "0    117.453835     57     0.410990  0.207584        3301.781785   \n",
       "1     78.302557     38     0.360462  0.138195        2765.747194   \n",
       "2    107.666016     50     0.454810  0.159711        3279.877315   \n",
       "3    103.359375     52     0.454698  0.223419        3124.176350   \n",
       "4    103.359375     51     0.451668  0.240985        3236.413205   \n",
       "..          ...    ...          ...       ...                ...   \n",
       "195  143.554688     69     0.269065  0.038438        1218.290508   \n",
       "196  117.453835     57     0.279421  0.020732        1284.921775   \n",
       "197  151.999081     76     0.281670  0.048189        1322.026416   \n",
       "198  135.999178     67     0.254774  0.034038        1516.626219   \n",
       "199  112.347147     25     0.245491  0.021982        1001.400439   \n",
       "\n",
       "     spectral_bandwidth      rolloff  zero_crossing_rate       mfcc1  \\\n",
       "0           3175.656592  7302.597509            0.144527  -40.802069   \n",
       "1           3222.713605  6873.391771            0.096015 -119.320605   \n",
       "2           3026.391118  6953.099622            0.172819  -35.277026   \n",
       "3           2970.320532  6618.134981            0.142721  -42.757179   \n",
       "4           2953.596262  6698.776160            0.164825    4.821123   \n",
       "..                  ...          ...                 ...         ...   \n",
       "195         1353.616224  2295.177788            0.069813 -300.937697   \n",
       "196         1659.622614  2476.568358            0.063750 -360.821769   \n",
       "197         1590.233329  2473.626709            0.076126 -274.426858   \n",
       "198         1629.500705  2973.782018            0.078788 -292.128300   \n",
       "199         1151.028078  1731.722862            0.057948 -369.184224   \n",
       "\n",
       "          mfcc2  ...    mfcc12    mfcc13    mfcc14    mfcc15    mfcc16  \\\n",
       "0     61.522054  ... -0.568772  0.552448 -1.981371  1.915755  0.076505   \n",
       "1     85.941142  ...  1.566938 -2.932391 -1.609138 -0.200545 -2.429376   \n",
       "2     61.242109  ... -0.518891  0.212918 -1.294660 -0.105626 -1.245879   \n",
       "3     62.799474  ...  3.020176 -1.242321 -3.524069 -0.325165 -1.443294   \n",
       "4     64.495215  ...  1.753352  3.336132  2.352307  1.428845 -0.604926   \n",
       "..          ...  ...       ...       ...       ...       ...       ...   \n",
       "195  161.111154  ... -1.808325 -4.104602 -2.045668 -1.596437  0.646648   \n",
       "196  138.999302  ...  4.684600  3.343187 -2.251255 -1.852868  2.215029   \n",
       "197  148.374498  ... -4.592368 -1.540610 -3.028737 -4.697145 -7.715005   \n",
       "198  127.769046  ...  4.709107  0.377206  1.464333  3.393674  6.076538   \n",
       "199  171.986997  ... -9.797995 -5.051084 -5.084386 -3.038442 -1.718458   \n",
       "\n",
       "       mfcc17    mfcc18    mfcc19    mfcc20  label  \n",
       "0   -1.270496  1.233082  1.493700  0.949931      0  \n",
       "1    1.207988  5.848729  1.609469  7.580263      0  \n",
       "2   -2.528455  0.571032  1.569434 -1.724873      0  \n",
       "3   -1.864617 -0.031307 -0.729631 -0.987611      0  \n",
       "4   -2.185691 -1.602394 -0.612146 -4.050126      0  \n",
       "..        ...       ...       ...       ...    ...  \n",
       "195  1.454414 -0.221094 -1.292084 -7.100527      1  \n",
       "196  1.784788 -1.519674 -3.253521  3.998950      1  \n",
       "197 -7.212517 -5.410751 -5.246985 -5.270150      1  \n",
       "198  0.963429 -0.753660 -2.105312  0.755132      1  \n",
       "199 -1.849090 -2.257478 -1.273307 -0.441167      1  \n",
       "\n",
       "[200 rows x 29 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df.drop(['filename'], axis=1)\n",
    "df['label'] = df['label'].replace({1:0, 2:1})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply escalation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(df.drop('label', axis=1)), columns=['tempo', 'beats', 'chroma_stft', 'rmse', 'spectral_centroid',\n",
    "       'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2',\n",
    "       'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10',\n",
    "       'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17',\n",
    "       'mfcc18', 'mfcc19', 'mfcc20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>beats</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.673025</td>\n",
       "      <td>0.515125</td>\n",
       "      <td>0.738691</td>\n",
       "      <td>0.872138</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>0.569193</td>\n",
       "      <td>0.886753</td>\n",
       "      <td>0.184110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447715</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>0.330628</td>\n",
       "      <td>0.536896</td>\n",
       "      <td>0.546841</td>\n",
       "      <td>0.411119</td>\n",
       "      <td>0.482866</td>\n",
       "      <td>0.446273</td>\n",
       "      <td>0.460374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.525061</td>\n",
       "      <td>0.338444</td>\n",
       "      <td>0.587080</td>\n",
       "      <td>0.890157</td>\n",
       "      <td>0.772558</td>\n",
       "      <td>0.324810</td>\n",
       "      <td>0.750568</td>\n",
       "      <td>0.338231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510471</td>\n",
       "      <td>0.559844</td>\n",
       "      <td>0.342998</td>\n",
       "      <td>0.443036</td>\n",
       "      <td>0.448375</td>\n",
       "      <td>0.525498</td>\n",
       "      <td>0.651598</td>\n",
       "      <td>0.451131</td>\n",
       "      <td>0.799144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.801348</td>\n",
       "      <td>0.393230</td>\n",
       "      <td>0.732496</td>\n",
       "      <td>0.814983</td>\n",
       "      <td>0.782612</td>\n",
       "      <td>0.711720</td>\n",
       "      <td>0.896336</td>\n",
       "      <td>0.182343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449181</td>\n",
       "      <td>0.681008</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>0.447246</td>\n",
       "      <td>0.494879</td>\n",
       "      <td>0.353066</td>\n",
       "      <td>0.458664</td>\n",
       "      <td>0.449451</td>\n",
       "      <td>0.323707</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.801018</td>\n",
       "      <td>0.555445</td>\n",
       "      <td>0.688457</td>\n",
       "      <td>0.793513</td>\n",
       "      <td>0.740358</td>\n",
       "      <td>0.560097</td>\n",
       "      <td>0.883362</td>\n",
       "      <td>0.192172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553174</td>\n",
       "      <td>0.624949</td>\n",
       "      <td>0.279363</td>\n",
       "      <td>0.437509</td>\n",
       "      <td>0.487122</td>\n",
       "      <td>0.383701</td>\n",
       "      <td>0.436645</td>\n",
       "      <td>0.352958</td>\n",
       "      <td>0.361377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.792145</td>\n",
       "      <td>0.600173</td>\n",
       "      <td>0.720202</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.750531</td>\n",
       "      <td>0.671447</td>\n",
       "      <td>0.965884</td>\n",
       "      <td>0.202875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515949</td>\n",
       "      <td>0.801321</td>\n",
       "      <td>0.474641</td>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.520065</td>\n",
       "      <td>0.368884</td>\n",
       "      <td>0.379211</td>\n",
       "      <td>0.357889</td>\n",
       "      <td>0.204901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.257414</td>\n",
       "      <td>0.084441</td>\n",
       "      <td>0.149399</td>\n",
       "      <td>0.174462</td>\n",
       "      <td>0.195036</td>\n",
       "      <td>0.192815</td>\n",
       "      <td>0.435564</td>\n",
       "      <td>0.812669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411291</td>\n",
       "      <td>0.514688</td>\n",
       "      <td>0.328492</td>\n",
       "      <td>0.381127</td>\n",
       "      <td>0.569244</td>\n",
       "      <td>0.536870</td>\n",
       "      <td>0.429707</td>\n",
       "      <td>0.329351</td>\n",
       "      <td>0.049043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.287741</td>\n",
       "      <td>0.039354</td>\n",
       "      <td>0.168245</td>\n",
       "      <td>0.291635</td>\n",
       "      <td>0.217917</td>\n",
       "      <td>0.162272</td>\n",
       "      <td>0.331698</td>\n",
       "      <td>0.673109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602082</td>\n",
       "      <td>0.801593</td>\n",
       "      <td>0.321660</td>\n",
       "      <td>0.369754</td>\n",
       "      <td>0.630872</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.382235</td>\n",
       "      <td>0.247029</td>\n",
       "      <td>0.616161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>0.109267</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>0.265065</td>\n",
       "      <td>0.217546</td>\n",
       "      <td>0.224619</td>\n",
       "      <td>0.481545</td>\n",
       "      <td>0.732281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329484</td>\n",
       "      <td>0.613458</td>\n",
       "      <td>0.295823</td>\n",
       "      <td>0.243608</td>\n",
       "      <td>0.240681</td>\n",
       "      <td>0.136902</td>\n",
       "      <td>0.239991</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>0.142565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.215565</td>\n",
       "      <td>0.073236</td>\n",
       "      <td>0.233780</td>\n",
       "      <td>0.280101</td>\n",
       "      <td>0.280639</td>\n",
       "      <td>0.238028</td>\n",
       "      <td>0.450843</td>\n",
       "      <td>0.602229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602802</td>\n",
       "      <td>0.687337</td>\n",
       "      <td>0.445132</td>\n",
       "      <td>0.602443</td>\n",
       "      <td>0.782606</td>\n",
       "      <td>0.514212</td>\n",
       "      <td>0.410238</td>\n",
       "      <td>0.295220</td>\n",
       "      <td>0.450421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.188381</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.088055</td>\n",
       "      <td>0.096889</td>\n",
       "      <td>0.123958</td>\n",
       "      <td>0.133046</td>\n",
       "      <td>0.317194</td>\n",
       "      <td>0.881312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176519</td>\n",
       "      <td>0.478227</td>\n",
       "      <td>0.227512</td>\n",
       "      <td>0.317173</td>\n",
       "      <td>0.476310</td>\n",
       "      <td>0.384418</td>\n",
       "      <td>0.355264</td>\n",
       "      <td>0.330140</td>\n",
       "      <td>0.389297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tempo     beats  chroma_stft      rmse  spectral_centroid  \\\n",
       "0    0.333333  0.393939     0.673025  0.515125           0.738691   \n",
       "1    0.111111  0.202020     0.525061  0.338444           0.587080   \n",
       "2    0.277778  0.323232     0.801348  0.393230           0.732496   \n",
       "3    0.253333  0.343434     0.801018  0.555445           0.688457   \n",
       "4    0.253333  0.333333     0.792145  0.600173           0.720202   \n",
       "..        ...       ...          ...       ...                ...   \n",
       "195  0.481481  0.515152     0.257414  0.084441           0.149399   \n",
       "196  0.333333  0.393939     0.287741  0.039354           0.168245   \n",
       "197  0.529412  0.585859     0.294327  0.109267           0.178740   \n",
       "198  0.438596  0.494949     0.215565  0.073236           0.233780   \n",
       "199  0.304348  0.070707     0.188381  0.042539           0.088055   \n",
       "\n",
       "     spectral_bandwidth   rolloff  zero_crossing_rate     mfcc1     mfcc2  \\\n",
       "0              0.872138  0.826700            0.569193  0.886753  0.184110   \n",
       "1              0.890157  0.772558            0.324810  0.750568  0.338231   \n",
       "2              0.814983  0.782612            0.711720  0.896336  0.182343   \n",
       "3              0.793513  0.740358            0.560097  0.883362  0.192172   \n",
       "4              0.787109  0.750531            0.671447  0.965884  0.202875   \n",
       "..                  ...       ...                 ...       ...       ...   \n",
       "195            0.174462  0.195036            0.192815  0.435564  0.812669   \n",
       "196            0.291635  0.217917            0.162272  0.331698  0.673109   \n",
       "197            0.265065  0.217546            0.224619  0.481545  0.732281   \n",
       "198            0.280101  0.280639            0.238028  0.450843  0.602229   \n",
       "199            0.096889  0.123958            0.133046  0.317194  0.881312   \n",
       "\n",
       "     ...    mfcc12    mfcc13    mfcc14    mfcc15    mfcc16    mfcc17  \\\n",
       "0    ...  0.447715  0.694088  0.330628  0.536896  0.546841  0.411119   \n",
       "1    ...  0.510471  0.559844  0.342998  0.443036  0.448375  0.525498   \n",
       "2    ...  0.449181  0.681008  0.353448  0.447246  0.494879  0.353066   \n",
       "3    ...  0.553174  0.624949  0.279363  0.437509  0.487122  0.383701   \n",
       "4    ...  0.515949  0.801321  0.474641  0.515301  0.520065  0.368884   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "195  ...  0.411291  0.514688  0.328492  0.381127  0.569244  0.536870   \n",
       "196  ...  0.602082  0.801593  0.321660  0.369754  0.630872  0.552117   \n",
       "197  ...  0.329484  0.613458  0.295823  0.243608  0.240681  0.136902   \n",
       "198  ...  0.602802  0.687337  0.445132  0.602443  0.782606  0.514212   \n",
       "199  ...  0.176519  0.478227  0.227512  0.317173  0.476310  0.384418   \n",
       "\n",
       "       mfcc18    mfcc19    mfcc20  label  \n",
       "0    0.482866  0.446273  0.460374      0  \n",
       "1    0.651598  0.451131  0.799144      0  \n",
       "2    0.458664  0.449451  0.323707      0  \n",
       "3    0.436645  0.352958  0.361377      0  \n",
       "4    0.379211  0.357889  0.204901      0  \n",
       "..        ...       ...       ...    ...  \n",
       "195  0.429707  0.329351  0.049043      1  \n",
       "196  0.382235  0.247029  0.616161      1  \n",
       "197  0.239991  0.163362  0.142565      1  \n",
       "198  0.410238  0.295220  0.450421      1  \n",
       "199  0.355264  0.330140  0.389297      1  \n",
       "\n",
       "[200 rows x 29 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df['label'] = df['label']\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(scaled_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>beats</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.130436</td>\n",
       "      <td>0.128691</td>\n",
       "      <td>0.427203</td>\n",
       "      <td>0.643917</td>\n",
       "      <td>0.489150</td>\n",
       "      <td>0.280028</td>\n",
       "      <td>0.540128</td>\n",
       "      <td>0.408824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302555</td>\n",
       "      <td>0.350238</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.311331</td>\n",
       "      <td>0.613025</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.643357</td>\n",
       "      <td>0.591753</td>\n",
       "      <td>0.602307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.110852</td>\n",
       "      <td>0.029235</td>\n",
       "      <td>0.034394</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.128514</td>\n",
       "      <td>0.776243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383133</td>\n",
       "      <td>0.423709</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.137367</td>\n",
       "      <td>0.250094</td>\n",
       "      <td>0.223975</td>\n",
       "      <td>0.483887</td>\n",
       "      <td>0.462275</td>\n",
       "      <td>0.327124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.414111</td>\n",
       "      <td>0.362151</td>\n",
       "      <td>0.367071</td>\n",
       "      <td>0.644048</td>\n",
       "      <td>0.443575</td>\n",
       "      <td>0.174314</td>\n",
       "      <td>0.661113</td>\n",
       "      <td>0.464350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357908</td>\n",
       "      <td>0.726386</td>\n",
       "      <td>0.175932</td>\n",
       "      <td>0.344081</td>\n",
       "      <td>0.253909</td>\n",
       "      <td>0.351863</td>\n",
       "      <td>0.375174</td>\n",
       "      <td>0.125970</td>\n",
       "      <td>0.670719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.359508</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>0.182984</td>\n",
       "      <td>0.178712</td>\n",
       "      <td>0.115137</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0.557955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356121</td>\n",
       "      <td>0.635142</td>\n",
       "      <td>0.230372</td>\n",
       "      <td>0.282924</td>\n",
       "      <td>0.448962</td>\n",
       "      <td>0.382224</td>\n",
       "      <td>0.426558</td>\n",
       "      <td>0.527105</td>\n",
       "      <td>0.572560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.298615</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>0.029083</td>\n",
       "      <td>0.117659</td>\n",
       "      <td>0.057838</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>0.092758</td>\n",
       "      <td>0.916233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212712</td>\n",
       "      <td>0.318974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044343</td>\n",
       "      <td>0.115634</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>0.294027</td>\n",
       "      <td>0.283224</td>\n",
       "      <td>0.294183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.128997</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.050543</td>\n",
       "      <td>0.036358</td>\n",
       "      <td>0.049146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.890222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319586</td>\n",
       "      <td>0.532182</td>\n",
       "      <td>0.254214</td>\n",
       "      <td>0.298588</td>\n",
       "      <td>0.354413</td>\n",
       "      <td>0.207509</td>\n",
       "      <td>0.198324</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.082085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.771512</td>\n",
       "      <td>0.574978</td>\n",
       "      <td>0.946656</td>\n",
       "      <td>0.935980</td>\n",
       "      <td>0.956478</td>\n",
       "      <td>0.918797</td>\n",
       "      <td>0.898242</td>\n",
       "      <td>0.050789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555782</td>\n",
       "      <td>0.765732</td>\n",
       "      <td>0.326938</td>\n",
       "      <td>0.394861</td>\n",
       "      <td>0.549905</td>\n",
       "      <td>0.577759</td>\n",
       "      <td>0.596276</td>\n",
       "      <td>0.473522</td>\n",
       "      <td>0.562003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.678397</td>\n",
       "      <td>0.501797</td>\n",
       "      <td>0.608823</td>\n",
       "      <td>0.819851</td>\n",
       "      <td>0.722838</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.833357</td>\n",
       "      <td>0.307799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425740</td>\n",
       "      <td>0.648240</td>\n",
       "      <td>0.263436</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.632668</td>\n",
       "      <td>0.656658</td>\n",
       "      <td>0.639236</td>\n",
       "      <td>0.543194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.188835</td>\n",
       "      <td>0.085624</td>\n",
       "      <td>0.166458</td>\n",
       "      <td>0.158456</td>\n",
       "      <td>0.187226</td>\n",
       "      <td>0.252940</td>\n",
       "      <td>0.403662</td>\n",
       "      <td>0.702518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454736</td>\n",
       "      <td>0.709956</td>\n",
       "      <td>0.434433</td>\n",
       "      <td>0.430742</td>\n",
       "      <td>0.492072</td>\n",
       "      <td>0.476749</td>\n",
       "      <td>0.492803</td>\n",
       "      <td>0.539058</td>\n",
       "      <td>0.568232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.363362</td>\n",
       "      <td>0.161669</td>\n",
       "      <td>0.314954</td>\n",
       "      <td>0.382934</td>\n",
       "      <td>0.330180</td>\n",
       "      <td>0.365119</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.566627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303031</td>\n",
       "      <td>0.465755</td>\n",
       "      <td>0.231545</td>\n",
       "      <td>0.328127</td>\n",
       "      <td>0.418362</td>\n",
       "      <td>0.281328</td>\n",
       "      <td>0.308160</td>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.351913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tempo     beats  chroma_stft      rmse  spectral_centroid  \\\n",
       "50   0.714286  0.676768     0.130436  0.128691           0.427203   \n",
       "192  0.583333  0.585859     0.110852  0.029235           0.034394   \n",
       "87   0.000000  0.101010     0.414111  0.362151           0.367071   \n",
       "145  0.481481  0.101010     0.359508  0.018045           0.123936   \n",
       "160  0.253333  0.303030     0.298615  0.007052           0.029083   \n",
       "..        ...       ...          ...       ...                ...   \n",
       "105  0.052632  0.060606     0.128997  0.003456           0.007103   \n",
       "36   0.304348  0.363636     0.771512  0.574978           0.946656   \n",
       "88   0.333333  0.373737     0.678397  0.501797           0.608823   \n",
       "162  0.583333  0.616162     0.188835  0.085624           0.166458   \n",
       "169  0.365079  0.393939     0.363362  0.161669           0.314954   \n",
       "\n",
       "     spectral_bandwidth   rolloff  zero_crossing_rate     mfcc1     mfcc2  \\\n",
       "50             0.643917  0.489150            0.280028  0.540128  0.408824   \n",
       "192            0.042373  0.061434            0.084668  0.128514  0.776243   \n",
       "87             0.644048  0.443575            0.174314  0.661113  0.464350   \n",
       "145            0.182984  0.178712            0.115137  0.213301  0.557955   \n",
       "160            0.117659  0.057838            0.025411  0.092758  0.916233   \n",
       "..                  ...       ...                 ...       ...       ...   \n",
       "105            0.050543  0.036358            0.049146  0.000000  0.890222   \n",
       "36             0.935980  0.956478            0.918797  0.898242  0.050789   \n",
       "88             0.819851  0.722838            0.470772  0.833357  0.307799   \n",
       "162            0.158456  0.187226            0.252940  0.403662  0.702518   \n",
       "169            0.382934  0.330180            0.365119  0.668900  0.566627   \n",
       "\n",
       "     ...    mfcc12    mfcc13    mfcc14    mfcc15    mfcc16    mfcc17  \\\n",
       "50   ...  0.302555  0.350238  0.086868  0.311331  0.613025  0.542786   \n",
       "192  ...  0.383133  0.423709  0.120763  0.137367  0.250094  0.223975   \n",
       "87   ...  0.357908  0.726386  0.175932  0.344081  0.253909  0.351863   \n",
       "145  ...  0.356121  0.635142  0.230372  0.282924  0.448962  0.382224   \n",
       "160  ...  0.212712  0.318974  0.000000  0.044343  0.115634  0.061160   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "105  ...  0.319586  0.532182  0.254214  0.298588  0.354413  0.207509   \n",
       "36   ...  0.555782  0.765732  0.326938  0.394861  0.549905  0.577759   \n",
       "88   ...  0.425740  0.648240  0.263436  0.410055  0.619147  0.632668   \n",
       "162  ...  0.454736  0.709956  0.434433  0.430742  0.492072  0.476749   \n",
       "169  ...  0.303031  0.465755  0.231545  0.328127  0.418362  0.281328   \n",
       "\n",
       "       mfcc18    mfcc19    mfcc20  label  \n",
       "50   0.643357  0.591753  0.602307      0  \n",
       "192  0.483887  0.462275  0.327124      1  \n",
       "87   0.375174  0.125970  0.670719      0  \n",
       "145  0.426558  0.527105  0.572560      1  \n",
       "160  0.294027  0.283224  0.294183      1  \n",
       "..        ...       ...       ...    ...  \n",
       "105  0.198324  0.038962  0.082085      1  \n",
       "36   0.596276  0.473522  0.562003      0  \n",
       "88   0.656658  0.639236  0.543194      0  \n",
       "162  0.492803  0.539058  0.568232      1  \n",
       "169  0.308160  0.189196  0.351913      1  \n",
       "\n",
       "[144 rows x 29 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>beats</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.101473</td>\n",
       "      <td>0.047336</td>\n",
       "      <td>0.419242</td>\n",
       "      <td>0.377267</td>\n",
       "      <td>0.418487</td>\n",
       "      <td>0.606771</td>\n",
       "      <td>0.391607</td>\n",
       "      <td>0.338105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946292</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.715095</td>\n",
       "      <td>0.714552</td>\n",
       "      <td>0.908716</td>\n",
       "      <td>0.929793</td>\n",
       "      <td>0.774175</td>\n",
       "      <td>0.501149</td>\n",
       "      <td>0.136961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.186988</td>\n",
       "      <td>0.061713</td>\n",
       "      <td>0.228285</td>\n",
       "      <td>0.257522</td>\n",
       "      <td>0.267314</td>\n",
       "      <td>0.275450</td>\n",
       "      <td>0.411151</td>\n",
       "      <td>0.608292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687876</td>\n",
       "      <td>0.818934</td>\n",
       "      <td>0.493929</td>\n",
       "      <td>0.533464</td>\n",
       "      <td>0.748492</td>\n",
       "      <td>0.519345</td>\n",
       "      <td>0.442995</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.294137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.279274</td>\n",
       "      <td>0.090641</td>\n",
       "      <td>0.189174</td>\n",
       "      <td>0.236720</td>\n",
       "      <td>0.242053</td>\n",
       "      <td>0.209611</td>\n",
       "      <td>0.460018</td>\n",
       "      <td>0.653671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509788</td>\n",
       "      <td>0.505352</td>\n",
       "      <td>0.296148</td>\n",
       "      <td>0.451695</td>\n",
       "      <td>0.765941</td>\n",
       "      <td>0.628365</td>\n",
       "      <td>0.541279</td>\n",
       "      <td>0.429630</td>\n",
       "      <td>0.396801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.855896</td>\n",
       "      <td>0.564985</td>\n",
       "      <td>0.730066</td>\n",
       "      <td>0.800970</td>\n",
       "      <td>0.773823</td>\n",
       "      <td>0.573385</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>0.179978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677675</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>0.427269</td>\n",
       "      <td>0.350515</td>\n",
       "      <td>0.605019</td>\n",
       "      <td>0.480026</td>\n",
       "      <td>0.508331</td>\n",
       "      <td>0.483214</td>\n",
       "      <td>0.689463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.602187</td>\n",
       "      <td>0.486142</td>\n",
       "      <td>0.711934</td>\n",
       "      <td>0.853088</td>\n",
       "      <td>0.791028</td>\n",
       "      <td>0.554368</td>\n",
       "      <td>0.878212</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540939</td>\n",
       "      <td>0.829094</td>\n",
       "      <td>0.476224</td>\n",
       "      <td>0.531221</td>\n",
       "      <td>0.450577</td>\n",
       "      <td>0.407691</td>\n",
       "      <td>0.332820</td>\n",
       "      <td>0.348211</td>\n",
       "      <td>0.279085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.251087</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.148150</td>\n",
       "      <td>0.245660</td>\n",
       "      <td>0.178429</td>\n",
       "      <td>0.146198</td>\n",
       "      <td>0.307556</td>\n",
       "      <td>0.766876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339175</td>\n",
       "      <td>0.524763</td>\n",
       "      <td>0.244338</td>\n",
       "      <td>0.290726</td>\n",
       "      <td>0.453754</td>\n",
       "      <td>0.325024</td>\n",
       "      <td>0.429668</td>\n",
       "      <td>0.494010</td>\n",
       "      <td>0.504786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.535653</td>\n",
       "      <td>0.463242</td>\n",
       "      <td>0.619855</td>\n",
       "      <td>0.527486</td>\n",
       "      <td>0.322214</td>\n",
       "      <td>0.905167</td>\n",
       "      <td>0.354056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561114</td>\n",
       "      <td>0.733162</td>\n",
       "      <td>0.475445</td>\n",
       "      <td>0.563945</td>\n",
       "      <td>0.586987</td>\n",
       "      <td>0.396980</td>\n",
       "      <td>0.428150</td>\n",
       "      <td>0.388322</td>\n",
       "      <td>0.433287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.877275</td>\n",
       "      <td>0.583795</td>\n",
       "      <td>0.531482</td>\n",
       "      <td>0.636191</td>\n",
       "      <td>0.580606</td>\n",
       "      <td>0.504358</td>\n",
       "      <td>0.974346</td>\n",
       "      <td>0.356442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377065</td>\n",
       "      <td>0.602937</td>\n",
       "      <td>0.385739</td>\n",
       "      <td>0.392258</td>\n",
       "      <td>0.477862</td>\n",
       "      <td>0.461277</td>\n",
       "      <td>0.500635</td>\n",
       "      <td>0.378176</td>\n",
       "      <td>0.212583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.102983</td>\n",
       "      <td>0.098173</td>\n",
       "      <td>0.147421</td>\n",
       "      <td>0.168391</td>\n",
       "      <td>0.171008</td>\n",
       "      <td>0.198489</td>\n",
       "      <td>0.425478</td>\n",
       "      <td>0.760923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623454</td>\n",
       "      <td>0.988446</td>\n",
       "      <td>0.536037</td>\n",
       "      <td>0.543172</td>\n",
       "      <td>0.663016</td>\n",
       "      <td>0.451907</td>\n",
       "      <td>0.334924</td>\n",
       "      <td>0.261695</td>\n",
       "      <td>0.479987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.376272</td>\n",
       "      <td>0.085587</td>\n",
       "      <td>0.236324</td>\n",
       "      <td>0.264277</td>\n",
       "      <td>0.250262</td>\n",
       "      <td>0.302509</td>\n",
       "      <td>0.530521</td>\n",
       "      <td>0.739055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298981</td>\n",
       "      <td>0.573423</td>\n",
       "      <td>0.246232</td>\n",
       "      <td>0.178492</td>\n",
       "      <td>0.348522</td>\n",
       "      <td>0.315312</td>\n",
       "      <td>0.315498</td>\n",
       "      <td>0.355342</td>\n",
       "      <td>0.423155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.624563</td>\n",
       "      <td>0.639959</td>\n",
       "      <td>0.637601</td>\n",
       "      <td>0.867059</td>\n",
       "      <td>0.773099</td>\n",
       "      <td>0.421031</td>\n",
       "      <td>0.880886</td>\n",
       "      <td>0.276597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544541</td>\n",
       "      <td>0.846845</td>\n",
       "      <td>0.491388</td>\n",
       "      <td>0.510594</td>\n",
       "      <td>0.614529</td>\n",
       "      <td>0.487144</td>\n",
       "      <td>0.396704</td>\n",
       "      <td>0.365173</td>\n",
       "      <td>0.492059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.673025</td>\n",
       "      <td>0.515125</td>\n",
       "      <td>0.738691</td>\n",
       "      <td>0.872138</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>0.569193</td>\n",
       "      <td>0.886753</td>\n",
       "      <td>0.184110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447715</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>0.330628</td>\n",
       "      <td>0.536896</td>\n",
       "      <td>0.546841</td>\n",
       "      <td>0.411119</td>\n",
       "      <td>0.482866</td>\n",
       "      <td>0.446273</td>\n",
       "      <td>0.460374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.134081</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.264110</td>\n",
       "      <td>0.250282</td>\n",
       "      <td>0.250179</td>\n",
       "      <td>0.386487</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>0.538058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525067</td>\n",
       "      <td>0.354313</td>\n",
       "      <td>0.347628</td>\n",
       "      <td>0.137797</td>\n",
       "      <td>0.580602</td>\n",
       "      <td>0.214049</td>\n",
       "      <td>0.127099</td>\n",
       "      <td>0.321211</td>\n",
       "      <td>0.686841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>0.868223</td>\n",
       "      <td>0.917351</td>\n",
       "      <td>0.896389</td>\n",
       "      <td>0.736177</td>\n",
       "      <td>0.880760</td>\n",
       "      <td>0.119780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503274</td>\n",
       "      <td>0.597789</td>\n",
       "      <td>0.381568</td>\n",
       "      <td>0.462308</td>\n",
       "      <td>0.465011</td>\n",
       "      <td>0.378983</td>\n",
       "      <td>0.399903</td>\n",
       "      <td>0.405192</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>0.612682</td>\n",
       "      <td>0.683399</td>\n",
       "      <td>0.794285</td>\n",
       "      <td>0.918026</td>\n",
       "      <td>0.872363</td>\n",
       "      <td>0.526351</td>\n",
       "      <td>0.876570</td>\n",
       "      <td>0.148473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516150</td>\n",
       "      <td>0.602501</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>0.314107</td>\n",
       "      <td>0.509739</td>\n",
       "      <td>0.291397</td>\n",
       "      <td>0.351956</td>\n",
       "      <td>0.374109</td>\n",
       "      <td>0.525111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.450326</td>\n",
       "      <td>0.484866</td>\n",
       "      <td>0.519262</td>\n",
       "      <td>0.746931</td>\n",
       "      <td>0.606192</td>\n",
       "      <td>0.362729</td>\n",
       "      <td>0.830319</td>\n",
       "      <td>0.410961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319923</td>\n",
       "      <td>0.671791</td>\n",
       "      <td>0.295322</td>\n",
       "      <td>0.444870</td>\n",
       "      <td>0.450199</td>\n",
       "      <td>0.503034</td>\n",
       "      <td>0.462766</td>\n",
       "      <td>0.466235</td>\n",
       "      <td>0.453006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.296233</td>\n",
       "      <td>0.198740</td>\n",
       "      <td>0.385684</td>\n",
       "      <td>0.636193</td>\n",
       "      <td>0.474006</td>\n",
       "      <td>0.220788</td>\n",
       "      <td>0.602412</td>\n",
       "      <td>0.437128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185452</td>\n",
       "      <td>0.574293</td>\n",
       "      <td>0.095731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.250042</td>\n",
       "      <td>0.306562</td>\n",
       "      <td>0.185594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>0.550087</td>\n",
       "      <td>0.022203</td>\n",
       "      <td>0.207586</td>\n",
       "      <td>0.305923</td>\n",
       "      <td>0.274776</td>\n",
       "      <td>0.194538</td>\n",
       "      <td>0.288448</td>\n",
       "      <td>0.607718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546228</td>\n",
       "      <td>0.814240</td>\n",
       "      <td>0.759313</td>\n",
       "      <td>0.919240</td>\n",
       "      <td>0.933423</td>\n",
       "      <td>0.751609</td>\n",
       "      <td>0.566837</td>\n",
       "      <td>0.244238</td>\n",
       "      <td>0.224089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.566546</td>\n",
       "      <td>0.225869</td>\n",
       "      <td>0.405928</td>\n",
       "      <td>0.679973</td>\n",
       "      <td>0.533565</td>\n",
       "      <td>0.182148</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.393187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501292</td>\n",
       "      <td>0.512843</td>\n",
       "      <td>0.292519</td>\n",
       "      <td>0.465016</td>\n",
       "      <td>0.404278</td>\n",
       "      <td>0.340240</td>\n",
       "      <td>0.227339</td>\n",
       "      <td>0.164720</td>\n",
       "      <td>0.406930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.192612</td>\n",
       "      <td>0.092511</td>\n",
       "      <td>0.245713</td>\n",
       "      <td>0.260596</td>\n",
       "      <td>0.274115</td>\n",
       "      <td>0.305669</td>\n",
       "      <td>0.442012</td>\n",
       "      <td>0.618081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595762</td>\n",
       "      <td>0.739508</td>\n",
       "      <td>0.459216</td>\n",
       "      <td>0.393810</td>\n",
       "      <td>0.586701</td>\n",
       "      <td>0.424087</td>\n",
       "      <td>0.384240</td>\n",
       "      <td>0.410273</td>\n",
       "      <td>0.471103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.602415</td>\n",
       "      <td>0.486016</td>\n",
       "      <td>0.711678</td>\n",
       "      <td>0.853213</td>\n",
       "      <td>0.791001</td>\n",
       "      <td>0.554143</td>\n",
       "      <td>0.878260</td>\n",
       "      <td>0.207816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541101</td>\n",
       "      <td>0.828711</td>\n",
       "      <td>0.475752</td>\n",
       "      <td>0.530774</td>\n",
       "      <td>0.449540</td>\n",
       "      <td>0.407595</td>\n",
       "      <td>0.332805</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.279511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.512784</td>\n",
       "      <td>0.427364</td>\n",
       "      <td>0.412876</td>\n",
       "      <td>0.773267</td>\n",
       "      <td>0.564386</td>\n",
       "      <td>0.101364</td>\n",
       "      <td>0.677170</td>\n",
       "      <td>0.335876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606185</td>\n",
       "      <td>0.791961</td>\n",
       "      <td>0.474378</td>\n",
       "      <td>0.615289</td>\n",
       "      <td>0.431096</td>\n",
       "      <td>0.349124</td>\n",
       "      <td>0.322725</td>\n",
       "      <td>0.333732</td>\n",
       "      <td>0.336377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>0.479404</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.815198</td>\n",
       "      <td>0.813688</td>\n",
       "      <td>0.783246</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>0.135305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552172</td>\n",
       "      <td>0.851382</td>\n",
       "      <td>0.491212</td>\n",
       "      <td>0.548554</td>\n",
       "      <td>0.585856</td>\n",
       "      <td>0.448157</td>\n",
       "      <td>0.451920</td>\n",
       "      <td>0.334241</td>\n",
       "      <td>0.439928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.987738</td>\n",
       "      <td>0.825380</td>\n",
       "      <td>0.843589</td>\n",
       "      <td>0.911999</td>\n",
       "      <td>0.887384</td>\n",
       "      <td>0.617240</td>\n",
       "      <td>0.888710</td>\n",
       "      <td>0.010052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540508</td>\n",
       "      <td>0.686521</td>\n",
       "      <td>0.450971</td>\n",
       "      <td>0.759520</td>\n",
       "      <td>0.867879</td>\n",
       "      <td>0.902505</td>\n",
       "      <td>0.692469</td>\n",
       "      <td>0.446189</td>\n",
       "      <td>0.354053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.361410</td>\n",
       "      <td>0.342746</td>\n",
       "      <td>0.219315</td>\n",
       "      <td>0.251386</td>\n",
       "      <td>0.252308</td>\n",
       "      <td>0.284026</td>\n",
       "      <td>0.750620</td>\n",
       "      <td>0.739942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504550</td>\n",
       "      <td>0.631158</td>\n",
       "      <td>0.315546</td>\n",
       "      <td>0.365997</td>\n",
       "      <td>0.594841</td>\n",
       "      <td>0.406010</td>\n",
       "      <td>0.475989</td>\n",
       "      <td>0.148421</td>\n",
       "      <td>0.310353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.203303</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.199858</td>\n",
       "      <td>0.240220</td>\n",
       "      <td>0.240139</td>\n",
       "      <td>0.207073</td>\n",
       "      <td>0.335493</td>\n",
       "      <td>0.610583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253798</td>\n",
       "      <td>0.356404</td>\n",
       "      <td>0.199076</td>\n",
       "      <td>0.446909</td>\n",
       "      <td>0.712027</td>\n",
       "      <td>0.883660</td>\n",
       "      <td>0.905784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.096747</td>\n",
       "      <td>0.209683</td>\n",
       "      <td>0.252871</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.220149</td>\n",
       "      <td>0.499201</td>\n",
       "      <td>0.667742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542822</td>\n",
       "      <td>0.488226</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.359045</td>\n",
       "      <td>0.661096</td>\n",
       "      <td>0.624879</td>\n",
       "      <td>0.613794</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.318501</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.572170</td>\n",
       "      <td>0.523892</td>\n",
       "      <td>0.690421</td>\n",
       "      <td>0.783760</td>\n",
       "      <td>0.732093</td>\n",
       "      <td>0.627260</td>\n",
       "      <td>0.913942</td>\n",
       "      <td>0.242757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457180</td>\n",
       "      <td>0.643615</td>\n",
       "      <td>0.255053</td>\n",
       "      <td>0.343618</td>\n",
       "      <td>0.320650</td>\n",
       "      <td>0.397093</td>\n",
       "      <td>0.462284</td>\n",
       "      <td>0.454523</td>\n",
       "      <td>0.452063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.854491</td>\n",
       "      <td>0.623182</td>\n",
       "      <td>0.845961</td>\n",
       "      <td>0.889702</td>\n",
       "      <td>0.859906</td>\n",
       "      <td>0.574722</td>\n",
       "      <td>0.818048</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620862</td>\n",
       "      <td>0.809376</td>\n",
       "      <td>0.413631</td>\n",
       "      <td>0.460263</td>\n",
       "      <td>0.499650</td>\n",
       "      <td>0.632515</td>\n",
       "      <td>0.490554</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.320140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.541364</td>\n",
       "      <td>0.491888</td>\n",
       "      <td>0.684740</td>\n",
       "      <td>0.801884</td>\n",
       "      <td>0.756386</td>\n",
       "      <td>0.611426</td>\n",
       "      <td>0.924873</td>\n",
       "      <td>0.246001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432742</td>\n",
       "      <td>0.765906</td>\n",
       "      <td>0.437999</td>\n",
       "      <td>0.495110</td>\n",
       "      <td>0.402822</td>\n",
       "      <td>0.510896</td>\n",
       "      <td>0.419633</td>\n",
       "      <td>0.413311</td>\n",
       "      <td>0.497222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.670090</td>\n",
       "      <td>0.316052</td>\n",
       "      <td>0.324429</td>\n",
       "      <td>0.447581</td>\n",
       "      <td>0.368576</td>\n",
       "      <td>0.271094</td>\n",
       "      <td>0.739175</td>\n",
       "      <td>0.595010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578543</td>\n",
       "      <td>0.366712</td>\n",
       "      <td>0.464726</td>\n",
       "      <td>0.348241</td>\n",
       "      <td>0.485549</td>\n",
       "      <td>0.689129</td>\n",
       "      <td>0.309306</td>\n",
       "      <td>0.589016</td>\n",
       "      <td>0.248262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.517047</td>\n",
       "      <td>0.503957</td>\n",
       "      <td>0.673299</td>\n",
       "      <td>0.795635</td>\n",
       "      <td>0.744748</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>0.926056</td>\n",
       "      <td>0.252116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433375</td>\n",
       "      <td>0.769902</td>\n",
       "      <td>0.437939</td>\n",
       "      <td>0.529761</td>\n",
       "      <td>0.408618</td>\n",
       "      <td>0.488030</td>\n",
       "      <td>0.406649</td>\n",
       "      <td>0.422373</td>\n",
       "      <td>0.496683</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.206203</td>\n",
       "      <td>0.201250</td>\n",
       "      <td>0.361565</td>\n",
       "      <td>0.547117</td>\n",
       "      <td>0.426131</td>\n",
       "      <td>0.255947</td>\n",
       "      <td>0.568262</td>\n",
       "      <td>0.473086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525307</td>\n",
       "      <td>0.911594</td>\n",
       "      <td>0.673402</td>\n",
       "      <td>0.788118</td>\n",
       "      <td>0.625119</td>\n",
       "      <td>0.680068</td>\n",
       "      <td>0.482485</td>\n",
       "      <td>0.314767</td>\n",
       "      <td>0.222138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.340223</td>\n",
       "      <td>0.149840</td>\n",
       "      <td>0.139724</td>\n",
       "      <td>0.155695</td>\n",
       "      <td>0.175357</td>\n",
       "      <td>0.185164</td>\n",
       "      <td>0.573755</td>\n",
       "      <td>0.889833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531956</td>\n",
       "      <td>0.500125</td>\n",
       "      <td>0.408573</td>\n",
       "      <td>0.385957</td>\n",
       "      <td>0.615029</td>\n",
       "      <td>0.480297</td>\n",
       "      <td>0.402449</td>\n",
       "      <td>0.363116</td>\n",
       "      <td>0.091677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.801018</td>\n",
       "      <td>0.555445</td>\n",
       "      <td>0.688457</td>\n",
       "      <td>0.793513</td>\n",
       "      <td>0.740358</td>\n",
       "      <td>0.560097</td>\n",
       "      <td>0.883362</td>\n",
       "      <td>0.192172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553174</td>\n",
       "      <td>0.624949</td>\n",
       "      <td>0.279363</td>\n",
       "      <td>0.437509</td>\n",
       "      <td>0.487122</td>\n",
       "      <td>0.383701</td>\n",
       "      <td>0.436645</td>\n",
       "      <td>0.352958</td>\n",
       "      <td>0.361377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.805125</td>\n",
       "      <td>0.554063</td>\n",
       "      <td>0.693943</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.749342</td>\n",
       "      <td>0.524413</td>\n",
       "      <td>0.874422</td>\n",
       "      <td>0.137633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477832</td>\n",
       "      <td>0.765955</td>\n",
       "      <td>0.210559</td>\n",
       "      <td>0.465565</td>\n",
       "      <td>0.504434</td>\n",
       "      <td>0.499365</td>\n",
       "      <td>0.483856</td>\n",
       "      <td>0.470998</td>\n",
       "      <td>0.566874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.252403</td>\n",
       "      <td>0.067252</td>\n",
       "      <td>0.121098</td>\n",
       "      <td>0.307928</td>\n",
       "      <td>0.157002</td>\n",
       "      <td>0.074646</td>\n",
       "      <td>0.240199</td>\n",
       "      <td>0.701044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440642</td>\n",
       "      <td>0.626266</td>\n",
       "      <td>0.317354</td>\n",
       "      <td>0.282939</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.344572</td>\n",
       "      <td>0.350573</td>\n",
       "      <td>0.409721</td>\n",
       "      <td>0.548135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.712507</td>\n",
       "      <td>0.662394</td>\n",
       "      <td>0.850255</td>\n",
       "      <td>0.899339</td>\n",
       "      <td>0.898406</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.942979</td>\n",
       "      <td>0.060884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445126</td>\n",
       "      <td>0.579206</td>\n",
       "      <td>0.347545</td>\n",
       "      <td>0.285039</td>\n",
       "      <td>0.478669</td>\n",
       "      <td>0.359962</td>\n",
       "      <td>0.439709</td>\n",
       "      <td>0.342042</td>\n",
       "      <td>0.278904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.301658</td>\n",
       "      <td>0.092367</td>\n",
       "      <td>0.238713</td>\n",
       "      <td>0.217001</td>\n",
       "      <td>0.263539</td>\n",
       "      <td>0.314419</td>\n",
       "      <td>0.479950</td>\n",
       "      <td>0.586717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228756</td>\n",
       "      <td>0.474222</td>\n",
       "      <td>0.242997</td>\n",
       "      <td>0.483639</td>\n",
       "      <td>0.799941</td>\n",
       "      <td>0.742331</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.298295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.652129</td>\n",
       "      <td>0.484396</td>\n",
       "      <td>0.620828</td>\n",
       "      <td>0.808638</td>\n",
       "      <td>0.716678</td>\n",
       "      <td>0.452206</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>0.263053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428938</td>\n",
       "      <td>0.569139</td>\n",
       "      <td>0.405151</td>\n",
       "      <td>0.404934</td>\n",
       "      <td>0.510729</td>\n",
       "      <td>0.412098</td>\n",
       "      <td>0.352843</td>\n",
       "      <td>0.543487</td>\n",
       "      <td>0.427511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tempo     beats  chroma_stft      rmse  spectral_centroid  \\\n",
       "142  0.209877  0.272727     0.101473  0.047336           0.419242   \n",
       "148  0.253333  0.323232     0.186988  0.061713           0.228285   \n",
       "171  0.888889  0.888889     0.279274  0.090641           0.189174   \n",
       "32   0.277778  0.363636     0.855896  0.564985           0.730066   \n",
       "22   0.253333  0.313131     0.602187  0.486142           0.711934   \n",
       "112  0.529412  0.474747     0.251087  0.028645           0.148150   \n",
       "60   0.277778  0.353535     0.924603  0.535653           0.463242   \n",
       "17   0.333333  0.363636     0.877275  0.583795           0.531482   \n",
       "134  0.400000  0.383838     0.102983  0.098173           0.147421   \n",
       "166  0.277778  0.343434     0.376272  0.085587           0.236324   \n",
       "5    0.230769  0.282828     0.624563  0.639959           0.637601   \n",
       "0    0.333333  0.393939     0.673025  0.515125           0.738691   \n",
       "187  0.438596  0.494949     0.134081  0.196700           0.264110   \n",
       "7    0.253333  0.303030     0.621528  0.523166           0.868223   \n",
       "68   0.400000  0.434343     0.612682  0.683399           0.794285   \n",
       "12   0.098039  0.181818     0.450326  0.484866           0.519262   \n",
       "48   0.277778  0.343434     0.296233  0.198740           0.385684   \n",
       "128  0.714286  0.707071     0.550087  0.022203           0.207586   \n",
       "14   0.190476  0.242424     0.566546  0.225869           0.405928   \n",
       "163  0.438596  0.404040     0.192612  0.092511           0.245713   \n",
       "94   0.253333  0.313131     0.602415  0.486016           0.711678   \n",
       "15   0.481481  0.515152     0.512784  0.427364           0.412876   \n",
       "64   0.230769  0.323232     0.946930  0.479404           0.795322   \n",
       "57   0.253333  0.333333     0.987738  0.825380           0.843589   \n",
       "167  0.888889  0.777778     0.361410  0.342746           0.219315   \n",
       "135  0.333333  0.313131     0.203303  0.036842           0.199858   \n",
       "178  0.481481  0.484848     0.256000  0.096747           0.209683   \n",
       "41   0.190476  0.282828     0.572170  0.523892           0.690421   \n",
       "27   0.172414  0.262626     0.854491  0.623182           0.845961   \n",
       "61   0.209877  0.282828     0.541364  0.491888           0.684740   \n",
       "9    0.190476  0.272727     0.670090  0.316052           0.324429   \n",
       "46   0.209877  0.303030     0.517047  0.503957           0.673299   \n",
       "6    0.400000  0.454545     0.206203  0.201250           0.361565   \n",
       "115  0.400000  0.474747     0.340223  0.149840           0.139724   \n",
       "3    0.253333  0.343434     0.801018  0.555445           0.688457   \n",
       "29   0.333333  0.404040     0.805125  0.554063           0.693943   \n",
       "149  0.481481  0.040404     0.252403  0.067252           0.121098   \n",
       "75   0.333333  0.393939     0.712507  0.662394           0.850255   \n",
       "101  0.583333  0.474747     0.301658  0.092367           0.238713   \n",
       "65   0.644444  0.646465     0.652129  0.484396           0.620828   \n",
       "\n",
       "     spectral_bandwidth   rolloff  zero_crossing_rate     mfcc1     mfcc2  \\\n",
       "142            0.377267  0.418487            0.606771  0.391607  0.338105   \n",
       "148            0.257522  0.267314            0.275450  0.411151  0.608292   \n",
       "171            0.236720  0.242053            0.209611  0.460018  0.653671   \n",
       "32             0.800970  0.773823            0.573385  0.912570  0.179978   \n",
       "22             0.853088  0.791028            0.554368  0.878212  0.207800   \n",
       "112            0.245660  0.178429            0.146198  0.307556  0.766876   \n",
       "60             0.619855  0.527486            0.322214  0.905167  0.354056   \n",
       "17             0.636191  0.580606            0.504358  0.974346  0.356442   \n",
       "134            0.168391  0.171008            0.198489  0.425478  0.760923   \n",
       "166            0.264277  0.250262            0.302509  0.530521  0.739055   \n",
       "5              0.867059  0.773099            0.421031  0.880886  0.276597   \n",
       "0              0.872138  0.826700            0.569193  0.886753  0.184110   \n",
       "187            0.250282  0.250179            0.386487  0.597892  0.538058   \n",
       "7              0.917351  0.896389            0.736177  0.880760  0.119780   \n",
       "68             0.918026  0.872363            0.526351  0.876570  0.148473   \n",
       "12             0.746931  0.606192            0.362729  0.830319  0.410961   \n",
       "48             0.636193  0.474006            0.220788  0.602412  0.437128   \n",
       "128            0.305923  0.274776            0.194538  0.288448  0.607718   \n",
       "14             0.679973  0.533565            0.182148  0.664190  0.393187   \n",
       "163            0.260596  0.274115            0.305669  0.442012  0.618081   \n",
       "94             0.853213  0.791001            0.554143  0.878260  0.207816   \n",
       "15             0.773267  0.564386            0.101364  0.677170  0.335876   \n",
       "64             0.815198  0.813688            0.783246  0.917177  0.135305   \n",
       "57             0.911999  0.887384            0.617240  0.888710  0.010052   \n",
       "167            0.251386  0.252308            0.284026  0.750620  0.739942   \n",
       "135            0.240220  0.240139            0.207073  0.335493  0.610583   \n",
       "178            0.252871  0.263341            0.220149  0.499201  0.667742   \n",
       "41             0.783760  0.732093            0.627260  0.913942  0.242757   \n",
       "27             0.889702  0.859906            0.574722  0.818048  0.022748   \n",
       "61             0.801884  0.756386            0.611426  0.924873  0.246001   \n",
       "9              0.447581  0.368576            0.271094  0.739175  0.595010   \n",
       "46             0.795635  0.744748            0.590417  0.926056  0.252116   \n",
       "6              0.547117  0.426131            0.255947  0.568262  0.473086   \n",
       "115            0.155695  0.175357            0.185164  0.573755  0.889833   \n",
       "3              0.793513  0.740358            0.560097  0.883362  0.192172   \n",
       "29             0.811166  0.749342            0.524413  0.874422  0.137633   \n",
       "149            0.307928  0.157002            0.074646  0.240199  0.701044   \n",
       "75             0.899339  0.898406            0.681548  0.942979  0.060884   \n",
       "101            0.217001  0.263539            0.314419  0.479950  0.586717   \n",
       "65             0.808638  0.716678            0.452206  0.866460  0.263053   \n",
       "\n",
       "     ...    mfcc12    mfcc13    mfcc14    mfcc15    mfcc16    mfcc17  \\\n",
       "142  ...  0.946292  0.807857  0.715095  0.714552  0.908716  0.929793   \n",
       "148  ...  0.687876  0.818934  0.493929  0.533464  0.748492  0.519345   \n",
       "171  ...  0.509788  0.505352  0.296148  0.451695  0.765941  0.628365   \n",
       "32   ...  0.677675  0.863584  0.427269  0.350515  0.605019  0.480026   \n",
       "22   ...  0.540939  0.829094  0.476224  0.531221  0.450577  0.407691   \n",
       "112  ...  0.339175  0.524763  0.244338  0.290726  0.453754  0.325024   \n",
       "60   ...  0.561114  0.733162  0.475445  0.563945  0.586987  0.396980   \n",
       "17   ...  0.377065  0.602937  0.385739  0.392258  0.477862  0.461277   \n",
       "134  ...  0.623454  0.988446  0.536037  0.543172  0.663016  0.451907   \n",
       "166  ...  0.298981  0.573423  0.246232  0.178492  0.348522  0.315312   \n",
       "5    ...  0.544541  0.846845  0.491388  0.510594  0.614529  0.487144   \n",
       "0    ...  0.447715  0.694088  0.330628  0.536896  0.546841  0.411119   \n",
       "187  ...  0.525067  0.354313  0.347628  0.137797  0.580602  0.214049   \n",
       "7    ...  0.503274  0.597789  0.381568  0.462308  0.465011  0.378983   \n",
       "68   ...  0.516150  0.602501  0.569562  0.314107  0.509739  0.291397   \n",
       "12   ...  0.319923  0.671791  0.295322  0.444870  0.450199  0.503034   \n",
       "48   ...  0.185452  0.574293  0.095731  0.000000  0.384922  0.112000   \n",
       "128  ...  0.546228  0.814240  0.759313  0.919240  0.933423  0.751609   \n",
       "14   ...  0.501292  0.512843  0.292519  0.465016  0.404278  0.340240   \n",
       "163  ...  0.595762  0.739508  0.459216  0.393810  0.586701  0.424087   \n",
       "94   ...  0.541101  0.828711  0.475752  0.530774  0.449540  0.407595   \n",
       "15   ...  0.606185  0.791961  0.474378  0.615289  0.431096  0.349124   \n",
       "64   ...  0.552172  0.851382  0.491212  0.548554  0.585856  0.448157   \n",
       "57   ...  0.540508  0.686521  0.450971  0.759520  0.867879  0.902505   \n",
       "167  ...  0.504550  0.631158  0.315546  0.365997  0.594841  0.406010   \n",
       "135  ...  0.253798  0.356404  0.199076  0.446909  0.712027  0.883660   \n",
       "178  ...  0.542822  0.488226  0.349782  0.359045  0.661096  0.624879   \n",
       "41   ...  0.457180  0.643615  0.255053  0.343618  0.320650  0.397093   \n",
       "27   ...  0.620862  0.809376  0.413631  0.460263  0.499650  0.632515   \n",
       "61   ...  0.432742  0.765906  0.437999  0.495110  0.402822  0.510896   \n",
       "9    ...  0.578543  0.366712  0.464726  0.348241  0.485549  0.689129   \n",
       "46   ...  0.433375  0.769902  0.437939  0.529761  0.408618  0.488030   \n",
       "6    ...  0.525307  0.911594  0.673402  0.788118  0.625119  0.680068   \n",
       "115  ...  0.531956  0.500125  0.408573  0.385957  0.615029  0.480297   \n",
       "3    ...  0.553174  0.624949  0.279363  0.437509  0.487122  0.383701   \n",
       "29   ...  0.477832  0.765955  0.210559  0.465565  0.504434  0.499365   \n",
       "149  ...  0.440642  0.626266  0.317354  0.282939  0.405700  0.344572   \n",
       "75   ...  0.445126  0.579206  0.347545  0.285039  0.478669  0.359962   \n",
       "101  ...  0.228756  0.474222  0.242997  0.483639  0.799941  0.742331   \n",
       "65   ...  0.428938  0.569139  0.405151  0.404934  0.510729  0.412098   \n",
       "\n",
       "       mfcc18    mfcc19    mfcc20  label  \n",
       "142  0.774175  0.501149  0.136961      1  \n",
       "148  0.442995  0.193200  0.294137      1  \n",
       "171  0.541279  0.429630  0.396801      1  \n",
       "32   0.508331  0.483214  0.689463      0  \n",
       "22   0.332820  0.348211  0.279085      0  \n",
       "112  0.429668  0.494010  0.504786      1  \n",
       "60   0.428150  0.388322  0.433287      0  \n",
       "17   0.500635  0.378176  0.212583      0  \n",
       "134  0.334924  0.261695  0.479987      1  \n",
       "166  0.315498  0.355342  0.423155      1  \n",
       "5    0.396704  0.365173  0.492059      0  \n",
       "0    0.482866  0.446273  0.460374      0  \n",
       "187  0.127099  0.321211  0.686841      1  \n",
       "7    0.399903  0.405192  0.392174      0  \n",
       "68   0.351956  0.374109  0.525111      0  \n",
       "12   0.462766  0.466235  0.453006      0  \n",
       "48   0.250042  0.306562  0.185594      0  \n",
       "128  0.566837  0.244238  0.224089      1  \n",
       "14   0.227339  0.164720  0.406930      0  \n",
       "163  0.384240  0.410273  0.471103      1  \n",
       "94   0.332805  0.348189  0.279511      0  \n",
       "15   0.322725  0.333732  0.336377      0  \n",
       "64   0.451920  0.334241  0.439928      0  \n",
       "57   0.692469  0.446189  0.354053      0  \n",
       "167  0.475989  0.148421  0.310353      1  \n",
       "135  0.905784  1.000000  1.000000      1  \n",
       "178  0.613794  0.457523  0.318501      1  \n",
       "41   0.462284  0.454523  0.452063      0  \n",
       "27   0.490554  0.403781  0.320140      0  \n",
       "61   0.419633  0.413311  0.497222      0  \n",
       "9    0.309306  0.589016  0.248262      0  \n",
       "46   0.406649  0.422373  0.496683      0  \n",
       "6    0.482485  0.314767  0.222138      0  \n",
       "115  0.402449  0.363116  0.091677      1  \n",
       "3    0.436645  0.352958  0.361377      0  \n",
       "29   0.483856  0.470998  0.566874      0  \n",
       "149  0.350573  0.409721  0.548135      1  \n",
       "75   0.439709  0.342042  0.278904      0  \n",
       "101  0.535900  0.298295  0.000000      1  \n",
       "65   0.352843  0.543487  0.427511      0  \n",
       "\n",
       "[40 rows x 29 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9563 - loss: 0.1608  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "best_accuracy = model.evaluate(test_df.drop('label', axis=1), to_categorical(test_df['label'], num_classes=2))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global best_accuracy\n",
    "    # Define the hyperparameters\n",
    "    units = trial.suggest_int('units', 32, 256)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a SimpleRNN layer with the suggested number of units\n",
    "    model.add(SimpleRNN(units, input_shape=(train_df.shape[1]-1, 1)))\n",
    "\n",
    "    # Add Dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model with 'categorical_crossentropy' loss, 'adam' optimizer with the suggested learning rate, and 'accuracy' metric\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_df.drop('label', axis=1), to_categorical(train_df['label'], num_classes=2), epochs=150, validation_data=(val_df.drop('label', axis=1),  to_categorical(val_df['label'], num_classes=2)), verbose=0)\n",
    "\n",
    "    # Save the model if it's the best so far\n",
    "    acuuracy = history.history['val_accuracy'][-1]\n",
    "    if  acuuracy > best_accuracy:\n",
    "        model.save('best_model.h5')\n",
    "        best_accuracy = acuuracy\n",
    "\n",
    "\n",
    "    # Return the validation accuracy\n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 09:28:17,747] A new study created in memory with name: no-name-153d92b6-3a79-4d1d-b3a3-a6cffdc6fa48\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "[I 2024-04-22 09:28:29,945] Trial 0 finished with value: 0.25 and parameters: {'units': 70, 'learning_rate': 0.005813163318480888}. Best is trial 0 with value: 0.25.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "[I 2024-04-22 09:28:41,893] Trial 1 finished with value: 1.0 and parameters: {'units': 43, 'learning_rate': 0.00508441282131872}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:28:54,660] Trial 2 finished with value: 0.25 and parameters: {'units': 170, 'learning_rate': 0.0027121773198452914}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:29:06,414] Trial 3 finished with value: 1.0 and parameters: {'units': 152, 'learning_rate': 0.0005618705976933972}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:29:19,377] Trial 4 finished with value: 0.25 and parameters: {'units': 184, 'learning_rate': 0.009044702613190111}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:29:36,545] Trial 5 finished with value: 0.25 and parameters: {'units': 85, 'learning_rate': 0.006474165225297961}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:29:55,480] Trial 6 finished with value: 0.25 and parameters: {'units': 129, 'learning_rate': 0.004811868803326894}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:30:15,232] Trial 7 finished with value: 0.25 and parameters: {'units': 158, 'learning_rate': 0.006509368916295997}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:30:32,474] Trial 8 finished with value: 0.25 and parameters: {'units': 46, 'learning_rate': 0.009817068404527413}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:30:52,587] Trial 9 finished with value: 0.25 and parameters: {'units': 158, 'learning_rate': 0.0030125162316730282}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:31:14,458] Trial 10 finished with value: 0.25 and parameters: {'units': 252, 'learning_rate': 0.008147444694832858}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:31:32,923] Trial 11 finished with value: 1.0 and parameters: {'units': 116, 'learning_rate': 0.00011711876983844325}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:31:53,171] Trial 12 finished with value: 1.0 and parameters: {'units': 207, 'learning_rate': 0.0006225815715095971}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:32:11,103] Trial 13 finished with value: 0.25 and parameters: {'units': 101, 'learning_rate': 0.0036357915328959056}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:32:21,483] Trial 14 finished with value: 1.0 and parameters: {'units': 33, 'learning_rate': 0.0022630962597852728}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:32:34,431] Trial 15 finished with value: 0.25 and parameters: {'units': 215, 'learning_rate': 0.004322596450907543}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:32:45,526] Trial 16 finished with value: 1.0 and parameters: {'units': 72, 'learning_rate': 0.0017794877728234243}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:32:56,623] Trial 17 finished with value: 0.25 and parameters: {'units': 136, 'learning_rate': 0.007248701901469897}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:33:08,695] Trial 18 finished with value: 0.25 and parameters: {'units': 193, 'learning_rate': 0.005366543238185816}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:33:21,482] Trial 19 finished with value: 1.0 and parameters: {'units': 236, 'learning_rate': 0.0011548002088914355}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:33:31,905] Trial 20 finished with value: 0.25 and parameters: {'units': 104, 'learning_rate': 0.003854538288899245}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:33:42,807] Trial 21 finished with value: 1.0 and parameters: {'units': 115, 'learning_rate': 0.00023757891141730608}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:33:53,892] Trial 22 finished with value: 1.0 and parameters: {'units': 54, 'learning_rate': 9.993627480193047e-05}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:34:05,164] Trial 23 finished with value: 1.0 and parameters: {'units': 145, 'learning_rate': 0.0014980698287359615}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:34:15,966] Trial 24 finished with value: 1.0 and parameters: {'units': 114, 'learning_rate': 0.0013101300355209359}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:34:26,477] Trial 25 finished with value: 1.0 and parameters: {'units': 88, 'learning_rate': 0.0031781068917079024}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:34:38,320] Trial 26 finished with value: 1.0 and parameters: {'units': 178, 'learning_rate': 0.0008510676263404446}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:34:49,372] Trial 27 finished with value: 0.9375 and parameters: {'units': 130, 'learning_rate': 0.002073185738779054}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:34:59,392] Trial 28 finished with value: 1.0 and parameters: {'units': 60, 'learning_rate': 0.0044691493704805555}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:35:10,776] Trial 29 finished with value: 0.875 and parameters: {'units': 74, 'learning_rate': 0.005215479953682196}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:35:21,833] Trial 30 finished with value: 0.25 and parameters: {'units': 151, 'learning_rate': 0.005789207079929196}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:35:34,921] Trial 31 finished with value: 1.0 and parameters: {'units': 210, 'learning_rate': 0.0006623948411934338}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:35:48,133] Trial 32 finished with value: 1.0 and parameters: {'units': 203, 'learning_rate': 0.00012289061647047955}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:36:00,681] Trial 33 finished with value: 0.25 and parameters: {'units': 228, 'learning_rate': 0.0023185094109854755}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:36:13,450] Trial 34 finished with value: 0.9375 and parameters: {'units': 170, 'learning_rate': 0.0008947530974907415}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:36:27,720] Trial 35 finished with value: 1.0 and parameters: {'units': 183, 'learning_rate': 0.0006099393558370845}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:36:39,747] Trial 36 finished with value: 0.25 and parameters: {'units': 90, 'learning_rate': 0.006935107628449603}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:36:52,797] Trial 37 finished with value: 0.25 and parameters: {'units': 195, 'learning_rate': 0.002744550800870806}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:37:02,931] Trial 38 finished with value: 1.0 and parameters: {'units': 33, 'learning_rate': 0.0016236572432173526}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:37:14,365] Trial 39 finished with value: 0.25 and parameters: {'units': 120, 'learning_rate': 0.006010032077412086}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:37:28,620] Trial 40 finished with value: 0.25 and parameters: {'units': 245, 'learning_rate': 0.008676961896168315}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:37:39,011] Trial 41 finished with value: 1.0 and parameters: {'units': 34, 'learning_rate': 0.0025135089439957082}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:37:51,350] Trial 42 finished with value: 1.0 and parameters: {'units': 49, 'learning_rate': 0.0005628205447485322}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:38:02,858] Trial 43 finished with value: 1.0 and parameters: {'units': 65, 'learning_rate': 0.0036338791146786056}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:38:13,922] Trial 44 finished with value: 1.0 and parameters: {'units': 37, 'learning_rate': 0.0016985270520409363}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:38:25,565] Trial 45 finished with value: 1.0 and parameters: {'units': 163, 'learning_rate': 0.0010734893054525478}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:38:36,871] Trial 46 finished with value: 1.0 and parameters: {'units': 83, 'learning_rate': 0.0020189727492508566}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:38:48,188] Trial 47 finished with value: 1.0 and parameters: {'units': 44, 'learning_rate': 2.593003462805169e-05}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:38:59,257] Trial 48 finished with value: 1.0 and parameters: {'units': 75, 'learning_rate': 0.0032667504174117325}. Best is trial 1 with value: 1.0.\n",
      "c:\\TEC\\M2\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2024-04-22 09:39:11,046] Trial 49 finished with value: 1.0 and parameters: {'units': 101, 'learning_rate': 0.004724919116033628}. Best is trial 1 with value: 1.0.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9563 - loss: 0.1608  \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model.h5')\n",
    "model.evaluate(test_df.drop('label', axis=1), to_categorical(test_df['label'], num_classes=2))\n",
    "\n",
    "predicted = model.predict(test_df.drop('label', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "predicted_labels = np.argmax(predicted, axis=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_df['label'], predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
